{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009d4f20",
   "metadata": {},
   "source": [
    "## SLDS Example Notebook\n",
    "### Overview\n",
    "This notebook illustrates how to use `jax_moseq.models.slds` to fit a sticky hierarchical Dirichlet process switching linear dynamical system (henceforth simply an \"SLDS\") to time series data via Gibbs sampling. Like the ARHMM model implemented by this package, this Bayesian nonparametric variant of the SLDS was originally described by Fox et al. (2008). Our version extends the original formulation by explicitly modeling the level of uncertainty for each observation, which allows us to downweight the influence of outliers when resampling the continuous latent states. For illustration purposes, we will simply fit the model to noisy observations derived from the depth principal components used in the ARHMM notebook.\n",
    "\n",
    "### Model\n",
    "#### Intuition\n",
    "The SLDS is a very natural extension of the ARHMM (see the ARHMM notebook for the needed background) in which the AR dynamics occur in a continuous latent space from which our observations are i.i.d. noisy emissions, rather than in the observation space directly. This allows the model to learn smoothly evolving low-dimensional latent dynamics and therefore creates an additional barrier between the inferred state sequences/AR parameters and the noise in the observations.\n",
    "\n",
    "#### Formalism\n",
    "The SLDS includes all the variables in the ARHMM (though it construes the continuous trajectories $X$ as a set of latent states to be inferred rather than as the observed variables) and the following additions (where $o$ denotes the observation dimensionality):\n",
    "\n",
    "- The continuous observations: $Y = \\{ y_t \\in \\mathbb{R}^{o} \\}_{t=1}^{T}$\n",
    "- The noise scales: $S = \\{ s_{t} \\in \\mathbb{R}_{+}^{o} \\}_{t=1}^{T}$\n",
    "- The emmission parameters: $C \\in \\mathbb{R}^{o \\times d}, d \\in \\mathbb{R}^{o}$\n",
    "- The unscaled noise: $\\sigma^2 \\in \\mathbb{R}_{+}^{o}$.\n",
    "\n",
    "Correspondingly, the generative model for the SLDS contains the following additions:\n",
    "\n",
    "- $y_t \\sim \\mathcal{N}(C x_{t} + d, S_t)$\n",
    "- $\\sigma_i^2 \\sim \\chi^{-2}(\\nu_{\\sigma}, \\sigma_0^2)$\n",
    "- $s_{t, i} \\sim \\chi^{-2} (\\nu_s, s_0)$\n",
    "\n",
    "Above the time $t$ noise covariance matrix $S_t = \\text{diag}(s_t \\odot \\sigma^2)$ is a diagonal matrix where $[S_t]_{ii} = s_{t, i} \\sigma_i^2$. Finally, we have the following hyperparameters:\n",
    "\n",
    "- The number of chi-squared degrees of freedom for the unscaled noise: $\\nu_{\\sigma} \\in \\mathbb{Z}_+$\n",
    "- The inverse chi-squared scaling factor for the unscaled noise: $\\sigma_0^2$\n",
    "- The number of chi-squared degrees of freedom for the noise scales: $\\nu_s \\in \\mathbb{Z}_+$\n",
    "- The inverse chi-squared scaling factor for the noise scales: $s_0$.\n",
    "\n",
    "\n",
    "#### Fitting\n",
    "We fit the SLDS via Gibbs sampling. Note that our current implementation does not resample the emission parameters $C, d$, which we fix in advance via PCA.\n",
    "\n",
    "### References\n",
    "[1] Fox, E., Sudderth, E., Jordan, M., & Willsky, A. (2008). Nonparametric Bayesian learning of switching linear dynamical systems. Advances in neural information processing systems, 21."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3c6565",
   "metadata": {},
   "source": [
    "### Code\n",
    "Before running this notebook, be sure to install `jax_moseq` and its associated dependencies. This notebook also requires `tqdm` and `matplotlib`. Also note that while a GPU is not required, it certainly doesn't hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8616f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_moseq.models import slds\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import trange\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a1257",
   "metadata": {},
   "source": [
    "#### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51343099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(d, depth=0, ind='  '):\n",
    "    if type(d) != dict:\n",
    "        try:\n",
    "            item = d.shape\n",
    "        except AttributeError:\n",
    "            item = d\n",
    "        print(f'{ind * depth}{item}')\n",
    "        return\n",
    "    \n",
    "    for k, v in d.items():\n",
    "        print(f'{ind * depth}{k}')\n",
    "        print_dict(v, depth + 1, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ll(key, ll_history):\n",
    "    plt.title(f'Log Likelihood of {key}')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Log Likelihood')\n",
    "    plt.plot(ll_history)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5fd332",
   "metadata": {},
   "source": [
    "#### Loading the Data\n",
    "The data is stored in a dictionary with two entries: \n",
    "- `'Y'` - a jax array of shape `(num_sessions, num_timesteps, obs_dim)` containing the continuous observations to which the model will be fit. In this case, these data are artificially derived from the mouse depth PCs (see Wiltschko et al. 2015).\n",
    "- `'mask'` - a jax array of shape `(num_sessions, num_timesteps)` indicating which data points are valid (which is useful in the event that data for each session differs in length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_path = 'example_data.npy'\n",
    "x = jax.device_put(np.load(x_path))\n",
    "\n",
    "latent_dim = x.shape[-1]\n",
    "obs_dim = 24\n",
    "projection_matrix = jr.normal(jr.PRNGKey(0), (obs_dim, latent_dim))\n",
    "\n",
    "Y = jnp.einsum('...d,od->...o', x, projection_matrix)\n",
    "del x\n",
    "\n",
    "data = {'Y': Y,\n",
    "        'mask': jnp.ones((Y.shape[:2]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ded543",
   "metadata": {},
   "source": [
    "#### Setting the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a13122",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 100\n",
    "nlags = 3\n",
    "\n",
    "# Note: we dub the dimensionality of the continuous\n",
    "# trajectories `latent_dim` despite the fact that they're\n",
    "# to harmonize the lingo across the ARHMM and SLDS.\n",
    "\n",
    "# TODO: identify a good set of hyperparameters for the dataset\n",
    "\n",
    "trans_hypparams = {\n",
    "    'gamma': 1e3, \n",
    "    'alpha': 5.7, \n",
    "    'kappa': 2e5,\n",
    "    'num_states': num_states}\n",
    "\n",
    "ar_hypparams = {\n",
    "    'S_0_scale': 10,\n",
    "    'K_0_scale': 0.1,\n",
    "    'latent_dim': latent_dim,\n",
    "    'num_states': num_states,\n",
    "    'nlags': nlags}\n",
    "\n",
    "obs_hypparams = {\n",
    "    'nu_sigma': 1e5,\n",
    "    'sigmasq_0': 10,\n",
    "    'nu_s': 5,\n",
    "    's_0': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885eaf75",
   "metadata": {},
   "source": [
    "#### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13365534",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = slds.init_model(data,\n",
    "                        trans_hypparams=trans_hypparams,\n",
    "                        ar_hypparams=ar_hypparams,\n",
    "                        obs_hypparams=obs_hypparams,\n",
    "                        verbose=True)\n",
    "\n",
    "print()\n",
    "print_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92337b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_iters = 50    # number of training iterations\n",
    "total_iters = 75\n",
    "\n",
    "ll_keys = ['z', 'x', 's', 'Y']\n",
    "ll_history = {key: [] for key in ll_keys}\n",
    "\n",
    "for i in trange(ar_iters):\n",
    "    # Perform Gibbs resampling\n",
    "    model = slds.resample_model(data, **model, ar_only=True)\n",
    "    \n",
    "    # Compute the likelihood of the data and\n",
    "    # resampled states given the resampled params\n",
    "    ll = slds.model_likelihood(data, **model)\n",
    "    for key in ll_keys:\n",
    "        ll_history[key].append(ll[key].item())\n",
    "        \n",
    "for i in trange(ar_iters, total_iters):\n",
    "    # Perform Gibbs resampling\n",
    "    model = slds.resample_model(data, **model)\n",
    "    \n",
    "    # Compute the likelihood of the data and\n",
    "    # resampled states given the resampled params\n",
    "    ll = slds.model_likelihood(data, **model)\n",
    "    for key in ll_keys:\n",
    "        ll_history[key].append(ll[key].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86305b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in ll_history.items():\n",
    "    plot_ll(k, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
